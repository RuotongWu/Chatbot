{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_ipynb\n",
    "import random\n",
    "import numpy as np \n",
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "from word_sequence import WordSequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE_THRESHOLD_CPU = 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting a list of GPU\n",
    "def _get_available_gpus():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return local_device_protos\n",
    "    #return [x.name for x in local_device_protos if x.device_type == 'GPU' ]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Deciding the use of GPU/CPU based on the data-size and memory size\n",
    "#If the data-size is too large, it will cosume a lof of GPU-memory\n",
    "def _get_embed_device(vocab_size):\n",
    "    gpus = _get_abailable_gpus()\n",
    "    if not gpus or vocab_size> VOCAB_SIZE_THRESHOLD_CPU:\n",
    "        return \"/cpu:0\"\n",
    "    return \"/gpu:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform sentences to vector using word_sequence\n",
    "def trans_sent(sentence, ws, max_len = None, add_end = False):\n",
    "    encoded = ws.transform(\n",
    "        sentence,\n",
    "        max_len = max_len if max_len is not None else len(sentence))\n",
    "    encoded_len = len(sentence) + (1 if add_end else 0)\n",
    "    if encoded_len > len(encoded):\n",
    "        encoded_len = len(encoded)\n",
    "    \n",
    "    return encoded, encoded_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_flow(data, ws, batch_sizes, raw = False, add_end = True):\n",
    "    all_data = list(zip(*data))\n",
    "    \n",
    "    if isinstance(ws,(list,tuple)):\n",
    "        assert len(ws) == len(data),'The lengh of word sequence is not equal to the length of data while word sequence is list or tuple'\n",
    "        \n",
    "    if isinstance(add_end, bool):\n",
    "        add_end = [add_end] * len(data)\n",
    "    else:\n",
    "        assert(isinstance(add_end,(list,tuple))),\"add_end must be bool, list or tuple\"\n",
    "        assert len(add_end) == len(data), \"The length of add_end should be equal as data when it is not bool\"\n",
    "        \n",
    "    \n",
    "    multiple = 2\n",
    "    if raw:\n",
    "        multiple = 3\n",
    "        \n",
    "    for d in data_batch:\n",
    "            for j in range(len(data)):\n",
    "                if isinstance(ws, (list, tuple)):\n",
    "                    w = ws[j]\n",
    "                else:\n",
    "                    w = ws\n",
    "\n",
    "                #添加结束标记（结尾）\n",
    "                line = d[j]\n",
    "                if add_end[j] and isinstance(line, (tuple, list)):\n",
    "                    line = list(line) + [WordSequence.END_TAG]\n",
    "                if w is not None:\n",
    "                    x, xl = transform_sentence(line, w, max_lens[j], add_end[j])\n",
    "                    batches[j * mul].append(x)\n",
    "                    batches[j * mul + 1].append(xl)\n",
    "                else:\n",
    "                    batches[j * mul].append(line)\n",
    "                    batches[j * mul + 1].append(line)\n",
    "                if raw:\n",
    "                    batches[j * mul + 2].append(line)\n",
    "        batches = [np.asarray(x) for x in batches]\n",
    "        yield batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_flow_bucket(data, ws, batch_size, raw=False, add_end=True,\n",
    "                      n_bucket=5, bucket_ind=1, debug=False):\n",
    "\n",
    "    all_data = list(zip(*data))\n",
    "    lengths = sorted(list(set([len(x[bucket_ind]) for x in all_data])))\n",
    "    if n_bucket > len(lengths):\n",
    "        n_bucket = len(lengths)\n",
    "\n",
    "    splits = np.array(lengths)[\n",
    "        (np.linspace(0, 1, 5, endpoint=False) * len(lengths)).astype(int)\n",
    "    ].tolist()\n",
    "\n",
    "    splits += [np.inf] \n",
    "\n",
    "    if debug:\n",
    "        print(splits)\n",
    "\n",
    "    ind_data = {}\n",
    "    for x in all_data:\n",
    "        l = len(x[bucket_ind])\n",
    "        for ind, s in enumerate(splits[:-1]):\n",
    "            if l >= s and l <= splits[ind + 1]:\n",
    "                if ind not in ind_data:\n",
    "                    ind_data[ind] = []\n",
    "                ind_data[ind].append(x)\n",
    "                break\n",
    "\n",
    "    inds = sorted(list(ind_data.keys()))\n",
    "    ind_p = [len(ind_data[x]) / len(all_data) for x in inds]\n",
    "    if debug:\n",
    "        print(np.sum(ind_p), ind_p)\n",
    "\n",
    "    if isinstance(ws, (list, tuple)):\n",
    "        assert len(ws) == len(data), 'The lengh of word sequence is not equal to the length of data while word sequence is list or tuple'\n",
    "\n",
    "    if isinstance(add_end, bool):\n",
    "        add_end = [add_end] * len(data)\n",
    "    else:\n",
    "        assert(isinstance(add_end, (list, tuple))), \"add_end must be bool, list or tuple\"\n",
    "        assert len(add_end) == len(data), \"The length of add_end should be equal as data when it is not bool\"\n",
    "\n",
    "    mul = 2\n",
    "    if raw:\n",
    "        mul = 3\n",
    "\n",
    "    while True:\n",
    "        choice_ind = np.random.choice(inds, p=ind_p)\n",
    "        if debug:\n",
    "            print('choice_ind', choice_ind)\n",
    "        data_batch = random.sample(ind_data[choice_ind], batch_size)\n",
    "        batches = [[] for i in range(len(data) * mul)]\n",
    "\n",
    "        max_lens = []\n",
    "        for j in range(len(data)):\n",
    "            max_len = max([\n",
    "                len(x[j]) if hasattr(x[j], '__len__') else 0\n",
    "                for x in data_batch\n",
    "            ]) + (1 if add_end[j] else 0)\n",
    "\n",
    "            max_lens.append(max_len)\n",
    "\n",
    "        for d in data_batch:\n",
    "            for j in range(len(data)):\n",
    "                if isinstance(ws, (list, tuple)):\n",
    "                    w = ws[j]\n",
    "                else:\n",
    "                    w = ws\n",
    "\n",
    "                line = d[j]\n",
    "                if add_end[j] and isinstance(line, (tuple, list)):\n",
    "                    line = list(line) + [WordSequence.END_TAG]\n",
    "\n",
    "                if w is not None:\n",
    "                    x, xl = transform_sentence(line, w, max_lens[j], add_end[j])\n",
    "                    batches[j * mul].append(x)\n",
    "                    batches[j * mul + 1].append(xl)\n",
    "                else:\n",
    "                    batches[j * mul].append(line)\n",
    "                    batches[j * mul + 1].append(line)\n",
    "                if raw:\n",
    "                    batches[j * mul + 2].append(line)\n",
    "        batches = [np.asarray(x) for x in batches]\n",
    "\n",
    "        yield batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatbotkernel",
   "language": "python",
   "name": "chatbotkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
