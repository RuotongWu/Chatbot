{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_ipynb\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import layers\n",
    "# from tensorflow.python.util import nest\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.contrib import seq2seq\n",
    "from tensorflow.contrib.seq2seq import BahdanauAttention\n",
    "from tensorflow.contrib.seq2seq import LuongAttention\n",
    "from tensorflow.contrib.seq2seq import AttentionWrapper\n",
    "from tensorflow.contrib.seq2seq import BeamSearchDecoder\n",
    "from tensorflow.contrib.rnn import LSTMCell\n",
    "from tensorflow.contrib.rnn import GRUCell\n",
    "from tensorflow.contrib.rnn import MultiRNNCell\n",
    "from tensorflow.contrib.rnn import DropoutWrapper\n",
    "from tensorflow.contrib.rnn import ResidualWrapper\n",
    "\n",
    "from word_sequence import WordSequence\n",
    "from data_utilis import get_embed_device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequence2Sequence(object):\n",
    "    def __init__(self,\n",
    "                 input_vocab_size,\n",
    "                 target_vocab_size,\n",
    "                 batch_size=32,\n",
    "                 embedding_size=300,\n",
    "                 mode='train',\n",
    "                 hidden_units=256,\n",
    "                 depth=1,\n",
    "                 beam_width=0,\n",
    "                 cell_type='lstm',\n",
    "                 dropout=0.2,\n",
    "                 use_dropout=False,\n",
    "                 use_residual=False,\n",
    "                 optimizer='adam',\n",
    "                 learning_rate=1e-3,\n",
    "                 min_learning_rate=1e-6,\n",
    "                 decay_steps=500000,\n",
    "                 max_gradient_norm=5.0,\n",
    "                 max_decode_step=None,\n",
    "                 attention_type='Bahdanau',\n",
    "                 bidirectional=False,\n",
    "                 time_major=False,\n",
    "                 seed=0,\n",
    "                 parallel_iterations=None,\n",
    "                 share_embedding=False,\n",
    "                 pretrained_embedding=False):\n",
    "        \n",
    "        \n",
    "        self.input_vocab_size = input_vocab_size\n",
    "        self.target_vocab_size = target_vocab_size\n",
    "        self.batch_size = batch_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.hidden_units = hidden_units\n",
    "        self.depth = depth\n",
    "        self.cell_type = cell_type.lower()\n",
    "        self.use_dropout = use_dropout\n",
    "        self.use_residual = use_residual\n",
    "        self.attention_type = attention_type\n",
    "        self.mode = mode\n",
    "        self.optimizer = optimizer\n",
    "        self.learning_rate = learning_rate\n",
    "        self.min_learning_rate = min_learning_rate\n",
    "        self.decay_steps = decay_steps\n",
    "        self.max_gradient_norm = max_gradient_norm\n",
    "        self.keep_prob = 1.0 - dropout\n",
    "        self.bidirectional = bidirectional\n",
    "        self.seed = seed\n",
    "        self.pretrained_embedding = pretrained_embedding\n",
    "        if isinstance(parallel_iterations, int):\n",
    "            self.parallel_iterations = parallel_iterations\n",
    "        else: # if parallel_iterations is None:\n",
    "            self.parallel_iterations = batch_size\n",
    "        self.time_major = time_major\n",
    "        self.share_embedding = share_embedding\n",
    "\n",
    "        self.initializer = tf.random_uniform_initializer(\n",
    "            -0.05, 0.05, dtype=tf.float32\n",
    "        )\n",
    "        # self.initializer = None\n",
    "\n",
    "        assert self.cell_type in ('gru', 'lstm'), 'cell_type should be gru or lstm'\n",
    "\n",
    "        if share_embedding:\n",
    "            assert input_vocab_size == target_vocab_size, 'the two vocb_size must be the same if share_embedding is true'\n",
    "\n",
    "        assert mode in ('train', 'decode'), 'mode must be train or decode'\n",
    "\n",
    "        assert dropout >= 0.0 and dropout < 1.0, '0 <= dropout < 1'\n",
    "\n",
    "        assert attention_type.lower() in ('bahdanau', 'luong'), \"attention_type must be bahdanau or luong\"\n",
    "\n",
    "        assert beam_width < target_vocab_size, \"beam_width should not bigger than target_vocab_size\"\n",
    "\n",
    "        self.keep_prob_placeholder = tf.placeholder(\n",
    "            tf.float32,\n",
    "            shape=[],\n",
    "            name='keep_prob'\n",
    "        )\n",
    "\n",
    "        self.global_step = tf.Variable(\n",
    "            0, trainable=False, name='global_step'\n",
    "        )\n",
    "\n",
    "        self.use_beamsearch_decode = False\n",
    "        self.beam_width = beam_width\n",
    "        self.use_beamsearch_decode = True if self.beam_width > 0 else False\n",
    "        self.max_decode_step = max_decode_step\n",
    "\n",
    "        assert self.optimizer.lower() in ('adadelta', 'adam', 'rmsprop', 'momentum', 'sgd'), 'optimizer should be one of adadelta, adam, rmsprop, momentum and sgd'\n",
    "\n",
    "        self.build_model()\n",
    "        \n",
    "    def build_model(self):\n",
    "        \n",
    "    def init_placeholders(self):\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatbotkernel",
   "language": "python",
   "name": "chatbotkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
